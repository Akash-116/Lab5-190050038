{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hRpsy5Zp8Lr"
      },
      "source": [
        "\n",
        "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
        "\n",
        "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
        "\n",
        "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
        "- Modify the code only between \n",
        "```\n",
        "## TODO\n",
        "## END TODO\n",
        "```\n",
        "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
        "- We will run the auto grading scripts with private test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mBBZWQn3WjsN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9j3in3odIle"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iaYuScGvdEum"
      },
      "outputs": [],
      "source": [
        "def preprocessing(X):\n",
        "  \"\"\"\n",
        "  Implement Normalization for input image features\n",
        "\n",
        "  Args:\n",
        "  X : numpy array of shape (n_samples, 784)\n",
        "   \n",
        "  Returns:\n",
        "  X_out: numpy array of shape (n_samples, 784) after normalization\n",
        "  \"\"\"\n",
        "  X_out = None\n",
        "  \n",
        "  ## TODO\n",
        "  \n",
        "  X_mean = np.mean(X, axis = 0)\n",
        "  X_std = np.std(X, axis = 0)\n",
        "  X_std[(X_std==0)] = 1\n",
        "  X_out = np.divide((X - X_mean), X_std)\n",
        "  \n",
        "  ## END TODO\n",
        "\n",
        "  assert X_out.shape == X.shape\n",
        "\n",
        "  return X_out\n",
        "\n",
        "\n",
        "def scaling(X):\n",
        "  \"\"\"\n",
        "  Implement MinMax Scaling on input image features\n",
        "\n",
        "  Args:\n",
        "  X : numpy array of shape (n_samples, 784)\n",
        "   \n",
        "  Returns:\n",
        "  X_scaled : numpy array of shape (n_samples, 784)\n",
        "  \"\"\"\n",
        "  X_scaled = None\n",
        "\n",
        "  ##TODO\n",
        "  # we will scale to 0-1\n",
        "  X_max = np.max(X, axis=0)\n",
        "  X_min = np.min(X, axis=0)\n",
        "  X_range = X_max - X_min\n",
        "  X_scaled = np.divide(X-X_min, X_range, where=(X_range != 0))\n",
        "\n",
        "  ##END TODO\n",
        "\n",
        "  assert X_scaled.shape == X.shape\n",
        "\n",
        "  return X_scaled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejROq-52YUol"
      },
      "source": [
        "### Split data into train/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "l07sJgZ3XG-N"
      },
      "outputs": [],
      "source": [
        "def split_data(X, Y, train_ratio=0.8):\n",
        "    '''\n",
        "    Split data into train and validation sets\n",
        "    The first floor(train_ratio*n_sample) samples form the train set\n",
        "    and the remaining the test set\n",
        "\n",
        "    Args:\n",
        "    X - numpy array of shape (n_samples, n_features)\n",
        "    Y - numpy array of shape (n_samples, 1)\n",
        "    train_ratio - fraction of samples to be used as training data\n",
        "\n",
        "    Returns:\n",
        "    X_train, Y_train, X_val, Y_val\n",
        "    '''\n",
        "    # Try Normalization and scaling and store it in X_transformed\n",
        "    X_transformed = X\n",
        "\n",
        "    ## TODO\n",
        "    X_transformed = scaling(preprocessing(X))\n",
        "    \n",
        "    ## END TODO\n",
        "\n",
        "    assert X_transformed.shape == X.shape\n",
        "\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "    num_train_samples = math.floor(num_samples * train_ratio)\n",
        "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
        "    val_indices = list(set(indices) - set(train_indices))\n",
        "    X_train, Y_train, X_val, Y_val = X_transformed[train_indices], Y[train_indices], X_transformed[val_indices], Y[val_indices]\n",
        "  \n",
        "    return X_train, Y_train, X_val, Y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FujjCCbMbsu4"
      },
      "source": [
        "#Flatten the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "hl8LxP1lAEiN"
      },
      "outputs": [],
      "source": [
        "class FlattenLayer:\n",
        "    '''\n",
        "    This class converts a multi-dimensional into 1-d vector\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_shape):\n",
        "        '''\n",
        "        Args:\n",
        "         input_shape : Original shape, tuple of ints\n",
        "        '''\n",
        "        self.input_shape = input_shape  \n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Converts a multi-dimensional into 1-d vector\n",
        "        Args:\n",
        "          input : training data, numpy array of shape (n_samples , self.input_shape)\n",
        "\n",
        "        Returns:\n",
        "          input: training data, numpy array of shape (n_samples , -1)\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        n_samples = input.shape[0]\n",
        "        ans = input.reshape(n_samples, -1)\n",
        "\n",
        "        # Modify the return statement to return flattened input\n",
        "        return ans\n",
        "        \n",
        "        # END TODO\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        '''\n",
        "        Converts back the passed array to original dimension \n",
        "        Args:\n",
        "        output_error :  numpy array \n",
        "        learning_rate: float\n",
        "\n",
        "        Returns:\n",
        "        output_error: A reshaped numpy array to allow backward pass\n",
        "        '''\n",
        "        # TODO\n",
        "        \n",
        "        n_samples = output_error.shape[0]\n",
        "        ans = output_error.reshape([n_samples]+list(self.input_shape))\n",
        "\n",
        "        # Modify the return statement to return reshaped array\n",
        "        return ans\n",
        "\n",
        "        # END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02MOHEdgh7T6"
      },
      "source": [
        "#Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oTrTMpTwtLXd"
      },
      "outputs": [],
      "source": [
        "class FCLayer:\n",
        "    '''\n",
        "    Implements a fully connected layer  \n",
        "    '''\n",
        "    def __init__(self, input_size, output_size):\n",
        "        '''\n",
        "        Args:\n",
        "         input_size : Input shape, int\n",
        "         output_size: Output shape, int \n",
        "        '''\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        ## TODO\n",
        "\n",
        "        #initilaise weights for this layer\n",
        "        self.weights = np.zeros((input_size, output_size))\n",
        "        # self.weights = np.random.rand(input_size, output_size)\n",
        "        \"\"\" W[i][j] is from ith input to jth output  \"\"\"\n",
        "\n",
        "        #initilaise bias for this layer\n",
        "        self.bias = np.random.rand(output_size)\n",
        "        \"\"\" B[j] is bias for jth output \"\"\"\n",
        "        \n",
        "        ## END TODO\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Performs a forward pass of a fully connected network\n",
        "        Args:\n",
        "          input : training data, numpy array of shape (n_samples , self.input_size)\n",
        "\n",
        "        Returns:\n",
        "           numpy array of shape (n_samples , self.output_size)\n",
        "        '''\n",
        "        ## TODO\n",
        "        self.input = input.copy() # for backtracking, this is the coefficent\n",
        "\n",
        "        ans = (input @ self.weights) + self.bias.reshape(1, self.output_size)\n",
        "\n",
        "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
        "        return ans\n",
        "\n",
        "        ## END TODO\n",
        "        \n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        '''\n",
        "        Performs a backward pass of a fully connected network along with updating the parameter \n",
        "        Args:\n",
        "          output_error :  numpy array \n",
        "          learning_rate: float\n",
        "\n",
        "        Returns:\n",
        "          Numpy array resulting from the backward pass\n",
        "        '''\n",
        "        ## TODO\n",
        "        n_samples = output_error.shape[0]\n",
        "        lamda = 1\n",
        "        \"\"\" Performing batch update. Hence, we stored the input_mean earlier \"\"\"\n",
        "        \n",
        "        ## Calculating the backpass value\n",
        "        assert output_error.shape[1] == self.output_size\n",
        "        backpass = output_error @ self.weights.T\n",
        "        assert backpass.shape[1] == self.input_size\n",
        "\n",
        "        ## Updating the parameters\n",
        "        assert self.weights.shape[0] == self.input.shape[1]\n",
        "        # batch update\n",
        "\n",
        "        self.weights -= learning_rate*(( self.input.T @ output_error) + (lamda/n_samples)*self.weights)\n",
        "        \n",
        "        self.bias -= learning_rate*(np.sum(output_error, axis=0).reshape(-1))\n",
        "\n",
        "        #Modify the return statement to return numpy array resulting from backward pass\n",
        "        self.bias = self.bias.reshape(-1)\n",
        "        assert self.bias.shape[0] == self.output_size, \"self.bias.shape[0] == {} != self.output_size == {}\".format(self.bias.shape[0], self.output_size)\n",
        "        return backpass\n",
        "        ## END TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "E6nSYAB2sam3"
      },
      "outputs": [],
      "source": [
        "class ActivationLayer:\n",
        "    '''\n",
        "    Implements a Activation layer which applies activation function on the inputs. \n",
        "    '''\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        '''\n",
        "          Args:\n",
        "          activation : Name of the activation function (sigmoid,tanh or relu)\n",
        "          activation_prime: Name of the corresponding function to be used during backpropagation (sigmoid_prime,tanh_prime or relu_prime)\n",
        "        '''\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "    \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Applies the activation function \n",
        "        Args:\n",
        "          input : numpy array on which activation function is to be applied\n",
        "\n",
        "        Returns:\n",
        "           numpy array output from the activation function\n",
        "        '''\n",
        "        ## TODO\n",
        "        self.input_size = input.shape[1]\n",
        "        self.output_size = input.shape[1]\n",
        "        self.input_diff = self.activation_prime(input)\n",
        "\n",
        "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
        "        return self.activation(input)\n",
        "\n",
        "\n",
        "        ## END TODO\n",
        "        \n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        '''\n",
        "        Performs a backward pass of a fully connected network along with updating the parameter \n",
        "        Args:\n",
        "          output_error :  numpy array \n",
        "          learning_rate: float\n",
        "\n",
        "        Returns:\n",
        "          Numpy array resulting from the backward pass\n",
        "        '''\n",
        "        ## TODO\n",
        "\n",
        "        ## Calculating the backpass value\n",
        "        assert output_error.shape == self.input_diff.shape\n",
        "        \n",
        "        backpass = np.multiply(self.input_diff, output_error)\n",
        "        assert backpass.shape == output_error.shape\n",
        "          \n",
        "        ## Nothing to update here\n",
        "\n",
        "        #Modify the return statement to return numpy array resulting from backward pass\n",
        "        return backpass\n",
        "        ## END TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RQeuIfkK3vyl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SoftmaxLayer:\n",
        "    '''\n",
        "      Implements a Softmax layer which applies softmax function on the inputs. \n",
        "    '''\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Applies the softmax function \n",
        "        Args:\n",
        "          input : numpy array on which softmax function is to be applied\n",
        "\n",
        "        Returns:\n",
        "           numpy array output from the softmax function\n",
        "        '''\n",
        "        ## TODO\n",
        "\n",
        "        ePowX = np.exp(input)\n",
        "        self.output = ePowX/(np.sum(ePowX, axis = 1).reshape(-1, 1))\n",
        "\n",
        "        #Modify the return statement to return numpy array\n",
        "        assert self.output.shape == input.shape\n",
        "        return self.output\n",
        "        \n",
        "        ## END TODO\n",
        "        \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        '''\n",
        "        Performs a backward pass of a Softmax layer\n",
        "        Args:\n",
        "          output_error :  numpy array \n",
        "          learning_rate: float\n",
        "\n",
        "        Returns:\n",
        "          Numpy array resulting from the backward pass\n",
        "        '''\n",
        "        ## TODO\n",
        "        \n",
        "        ## Calculating the backpass value\n",
        "        assert output_error.shape == self.output.shape\n",
        "\n",
        "        temp_k = np.multiply(self.output, output_error)\n",
        "        temp_k_sum = np.sum(temp_k, axis=1)\n",
        "\n",
        "        backpass = temp_k - np.multiply(self.output, temp_k_sum.reshape(-1, 1))\n",
        "\n",
        "        assert backpass.shape == output_error.shape\n",
        "\n",
        "        ## Nothing to update here also\n",
        "\n",
        "        #Modify the return statement to return numpy array resulting from backward pass\n",
        "        return backpass\n",
        "        ## END TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "LuPbn70Wt8Q7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    '''\n",
        "    Sigmoid function \n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying simoid function\n",
        "    '''\n",
        "    ## TODO\n",
        "    ans = 1/(1+(np.exp(-x)))\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "\n",
        "    ## END TODO\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    '''\n",
        "     Implements derivative of Sigmoid function, for the backward pass\n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying derivative of Sigmoid function\n",
        "    '''\n",
        "    ## TODO\n",
        "    ePowX = np.exp(x)\n",
        "    ans = np.divide(ePowX , np.square(1+ePowX))\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "\n",
        "    ## END TODO\n",
        "\n",
        "def tanh(x):\n",
        "    '''\n",
        "    Tanh function \n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying tanh function\n",
        "    '''\n",
        "    ## TODO\n",
        "    ePowX = np.exp(x)\n",
        "    ePowMinusX = np.exp(-x)\n",
        "\n",
        "    ans = np.divide((ePowX - ePowMinusX), (ePowX + ePowMinusX))\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "\n",
        "    ## END TODO\n",
        "\n",
        "def tanh_prime(x):\n",
        "    '''\n",
        "     Implements derivative of Tanh function, for the backward pass\n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying derivative of Tanh function\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    ePowX = np.exp(x)\n",
        "    ePowMinusX = np.exp(-x)\n",
        "\n",
        "    ans = (4)/np.square(ePowX + ePowMinusX)\n",
        "    \n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "    ## END TODO\n",
        "\n",
        "def relu(x):\n",
        "    '''\n",
        "    ReLU function \n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying ReLU function\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    ans = np.copy(x)\n",
        "    ans[ans < 0] = 0.0\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "    ## END TODO\n",
        "\n",
        "def relu_prime(x):\n",
        "    '''\n",
        "     Implements derivative of ReLU function, for the backward pass\n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying derivative of ReLU function\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    ans = np.ones(x.shape)\n",
        "    ans[x < 0] = 0.0\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert x.shape == ans.shape\n",
        "    return ans\n",
        "    \n",
        "\n",
        "    ## END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rXY7jkUzuqEk"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, output):\n",
        "    '''\n",
        "    MSE loss\n",
        "    Args:\n",
        "        y_true :  Ground truth labels, numpy array \n",
        "        y_true :  Predicted labels, numpy array .... not predicted labels\n",
        "    Returns:\n",
        "       loss : float\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    \"\"\" Mean over various samples \"\"\"\n",
        "    assert y_true.shape[0] == output.shape[0]\n",
        "\n",
        "    n_samples = output.shape[0]\n",
        "    ans = np.sum(np.square(output), axis = 1)+1\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        ans[i] -= 2*output[i][y_true[i]]\n",
        "    \n",
        "    ans = np.mean(ans)\n",
        "\n",
        "    #Modify the return statement to return a float\n",
        "    return ans\n",
        "    ## END TODO\n",
        "\n",
        "def mse_prime(y_true, output):\n",
        "    '''\n",
        "    Implements derivative of MSE function, for the backward pass\n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying derivative of MSE function\n",
        "    '''\n",
        "    ## TODO\n",
        "    assert y_true.shape[0] == output.shape[0]\n",
        "\n",
        "    n_samples = y_true.shape[0]\n",
        "\n",
        "    # ans = (2/n_samples)* (y_pred - y_true)\n",
        "    ans = output.copy() * 2\n",
        "    for i in range(n_samples):\n",
        "        ans[i][y_true[i]] -= 2\n",
        "    ans /= n_samples\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert ans.shape == output.shape\n",
        "    return ans\n",
        "    ## END TODO\n",
        "\n",
        "def cross_entropy(y_true, output):\n",
        "    '''\n",
        "    Cross entropy loss \n",
        "    Args:\n",
        "        y_true :  Ground truth labels, numpy array \n",
        "        y_true :  Predicted labels, numpy array \n",
        "    Returns:\n",
        "       loss : float\n",
        "    '''\n",
        "    ## TODO\n",
        "    assert y_true.shape == output.shape\n",
        "\n",
        "    n_samples = y_true.shape[0]\n",
        "\n",
        "    ans = -1* np.log(1-output)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        ans[i][y_true[i]] = -1*np.log(output[i][y_true[i]])\n",
        "    \n",
        "    ans = np.sum(ans, axis = 1)\n",
        "    ans = np.mean(ans)\n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    return ans\n",
        "    ## END TODO\n",
        "\n",
        "def cross_entropy_prime(y_true, output):\n",
        "    '''\n",
        "    Implements derivative of cross entropy function, for the backward pass\n",
        "    Args:\n",
        "        x :  numpy array \n",
        "    Returns:\n",
        "        Numpy array after applying derivative of cross entropy function\n",
        "    '''\n",
        "    ## TODO\n",
        "    assert y_true.shape[0] == output.shape[0]\n",
        "\n",
        "    n_samples = y_true.shape[0]\n",
        "\n",
        "    ans = 1/(1-output)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        ans[i][y_true[i]] = -1/(output[i][y_true[i]])\n",
        "\n",
        "    ans /= n_samples\n",
        "\n",
        "    \n",
        "\n",
        "    #Modify the return statement to return numpy array resulting from backward pass\n",
        "    assert ans.shape == output.shape\n",
        "    return ans\n",
        "    ## END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u23euUDztNtb"
      },
      "source": [
        "## Fit function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "-sCYdGN8tSdp"
      },
      "outputs": [],
      "source": [
        "def fit(X_train, Y_train,dataset_name):\n",
        "\n",
        "    '''\n",
        "    Create and trains a feedforward network\n",
        "\n",
        "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
        "    Args:\n",
        "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
        "        Y_train -- np array of share (num_test,) for flowers and (num_test,) for mnist.\n",
        "        dataset_name -- name of the dataset (flowers or mnist)\n",
        "    \n",
        "    '''\n",
        "     \n",
        "    #Note that this just a template to help you create your own feed forward network \n",
        "    ## TODO\n",
        "\n",
        "    X_train = scaling(preprocessing(X_train))\n",
        "\n",
        "    if(dataset_name == \"mnist\"):\n",
        "        #define your network\n",
        "        #This network would work only for mnist\n",
        "        network = [\n",
        "            FlattenLayer(input_shape=(28, 28)),\n",
        "            FCLayer(28 * 28, 20),\n",
        "            ActivationLayer(sigmoid, sigmoid_prime),\n",
        "            FCLayer(20, 10),\n",
        "            SoftmaxLayer(10)\n",
        "        ] # This creates feed forward \n",
        "\n",
        "\n",
        "        # Choose appropriate learning rate and no. of epoch\n",
        "        # epochs = 200\n",
        "        epochs = 60\n",
        "        learning_rate = 0.8\n",
        "\n",
        "        ## No regularisation considered for weights...\n",
        "\n",
        "        batch_size = 2000\n",
        "        n_samples = X_train.shape[0]\n",
        "\n",
        "        # Change training loop as you see fit - FINE!\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end_index = min(n_samples, i+batch_size)\n",
        "\n",
        "                x = X_train[i:end_index]\n",
        "                y_true = Y_train[i:end_index]\n",
        "\n",
        "                # forward\n",
        "                output = x\n",
        "                for layer in network:\n",
        "                    output = layer.forward(output)\n",
        "                \n",
        "                # error (display purpose only)\n",
        "                error += mse(y_true.reshape(-1,1), output)\n",
        "\n",
        "                # backward\n",
        "                output_error = cross_entropy_prime(y_true.reshape(-1, 1), output )\n",
        "                for layer in reversed(network):\n",
        "                    output_error = layer.backward(output_error, learning_rate)\n",
        "\n",
        "            error /= (n_samples/batch_size)\n",
        "            print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
        "\n",
        "\n",
        "        #Save you model weights\n",
        "        np.save(\"./models/{}_FC1_weights\".format(dataset_name), network[1].weights)\n",
        "        np.save(\"./models/{}_FC1_bias\".format(dataset_name), network[1].bias)\n",
        "        np.save(\"./models/{}_FC2_weights\".format(dataset_name), network[3].weights)\n",
        "        np.save(\"./models/{}_FC2_bias\".format(dataset_name), network[3].bias)\n",
        "\n",
        "    \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "    \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "    \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "    \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "\n",
        "    if(dataset_name == \"flowers\"):\n",
        "\n",
        "        #define your network\n",
        "        #This network would work only for flowers\n",
        "        network = [\n",
        "            FCLayer(2048, 20),\n",
        "            ActivationLayer(tanh, tanh_prime),\n",
        "            FCLayer(20, 5),\n",
        "            SoftmaxLayer(5)\n",
        "        ] # This creates feed forward \n",
        "\n",
        "\n",
        "        # Choose appropriate learning rate and no. of epoch\n",
        "        epochs = 100\n",
        "        learning_rate = 0.1\n",
        "\n",
        "        ## No regularisation considered for weights...\n",
        "\n",
        "        batch_size = 200\n",
        "        n_samples = X_train.shape[0]\n",
        "\n",
        "        # Change training loop as you see fit - FINE!\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end_index = min(n_samples, i+batch_size)\n",
        "\n",
        "                x = X_train[i:end_index]\n",
        "                y_true = Y_train[i:end_index]\n",
        "\n",
        "                # forward\n",
        "                output = x\n",
        "                for layer in network:\n",
        "                    output = layer.forward(output)\n",
        "                \n",
        "                # error (display purpose only)\n",
        "                error += mse(y_true.reshape(-1,1), output)\n",
        "\n",
        "                # backward\n",
        "                output_error = cross_entropy_prime(y_true.reshape(-1, 1), output )\n",
        "                for layer in reversed(network):\n",
        "                    output_error = layer.backward(output_error, learning_rate)\n",
        "\n",
        "            error /= (n_samples/batch_size)\n",
        "            print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
        "\n",
        "\n",
        "        #Save you model weights\n",
        "        np.save(\"./models/{}_FC1_weights\".format(dataset_name), network[0].weights)\n",
        "        np.save(\"./models/{}_FC1_bias\".format(dataset_name), network[0].bias)\n",
        "        np.save(\"./models/{}_FC2_weights\".format(dataset_name), network[2].weights)\n",
        "        np.save(\"./models/{}_FC2_bias\".format(dataset_name), network[2].bias)\n",
        "\n",
        "    \n",
        "    ## END TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QprSHht4iwe9"
      },
      "outputs": [],
      "source": [
        "def predict(X_test, dataset_name):\n",
        "  \"\"\"\n",
        "\n",
        "  X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
        "\n",
        "  This is the only function that we will call from the auto grader. \n",
        "\n",
        "  This function should only perform inference, please donot train your models here.\n",
        "  \n",
        "  Steps to be done here:\n",
        "  1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
        "  2. Ensure that you read weights using only the libraries we have given above.\n",
        "  3. Initialize your model with your trained weights\n",
        "  4. Compute the predicted labels and return it\n",
        "\n",
        "  Please provide us the complete code you used for training including any techniques\n",
        "  like data augmentation etc. that you have tried out. \n",
        "\n",
        "  Return:\n",
        "  Y_test - nparray of shape (num_test,)\n",
        "  \"\"\"\n",
        "  Y_test = np.zeros(X_test.shape[0],)\n",
        "\n",
        "  ## TODO\n",
        "  X_test = scaling(preprocessing(X_test))\n",
        "\n",
        "  if(dataset_name == 'mnist'):\n",
        "\n",
        "    #define your network\n",
        "    #This network would work only for mnist\n",
        "    network = [\n",
        "        FlattenLayer(input_shape=(28, 28)),\n",
        "        FCLayer(28 * 28, 20),\n",
        "        ActivationLayer(sigmoid, sigmoid_prime),\n",
        "        FCLayer(20, 10),\n",
        "        SoftmaxLayer(10)\n",
        "    ]  # This creates feed forward\n",
        "\n",
        "    network[1].weights = np.load(\"./models/{}_FC1_weights.npy\".format(dataset_name))\n",
        "    network[1].bias = np.load(\"./models/{}_FC1_bias.npy\".format(dataset_name) )\n",
        "    network[3].weights = np.load(\"./models/{}_FC2_weights.npy\".format(dataset_name))\n",
        "    network[3].bias = np.load(\"./models/{}_FC2_bias.npy\".format(dataset_name))\n",
        "\n",
        "    # forward\n",
        "    output = X_test\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "\n",
        "    Y_test = np.argmax(output, axis = 1)\n",
        "\n",
        "  \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "  \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "  \"\"\" aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \"\"\"\n",
        "    \n",
        "  if(dataset_name == 'flowers'):\n",
        "\n",
        "    #define your network\n",
        "    #This network would work only for flowers\n",
        "    network = [\n",
        "        FCLayer(2048, 20),\n",
        "        ActivationLayer(tanh, tanh_prime),\n",
        "        FCLayer(20, 5),\n",
        "        SoftmaxLayer(5)\n",
        "    ]  # This creates feed forward\n",
        "\n",
        "    network[0].weights = np.load(\"./models/{}_FC1_weights.npy\".format(dataset_name))\n",
        "    network[0].bias = np.load(\"./models/{}_FC1_bias.npy\".format(dataset_name) )\n",
        "    network[2].weights = np.load(\"./models/{}_FC2_weights.npy\".format(dataset_name))\n",
        "    network[2].bias = np.load(\"./models/{}_FC2_bias.npy\".format(dataset_name))\n",
        "\n",
        "    # forward\n",
        "    output = X_test\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "\n",
        "    Y_test = np.argmax(output, axis = 1)\n",
        "\n",
        "  ## END TODO\n",
        "  assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
        "  return Y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Pop_HsvuEZ"
      },
      "source": [
        "### Loading MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ttYbN2psvtu_"
      },
      "outputs": [],
      "source": [
        "# ## Loading MNIST dataset\n",
        "\n",
        "# dataset = \"mnist\" \n",
        "# with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
        "#     train_mnist = pkl.load(file)\n",
        "#     print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Splitting MNIST dataset\n",
        "\n",
        "# X_train, Y_train, X_val, Y_val = split_data(train_mnist[0], train_mnist[1], train_ratio=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Predict for MNIST\n",
        "\n",
        "# # X_train, Y_train, X_val, Y_val = split_data(train_mnist[0], train_mnist[1], train_ratio=0.2)\n",
        "\n",
        "\n",
        "# Y_test = predict(X_val, 'mnist')\n",
        "# accuracy = np.sum(Y_test == Y_val)/len(Y_test)\n",
        "# print(\"Predicting percentage accuracy :\", accuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading FLOWERS dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Loading FLOWERS dataset\n",
        "\n",
        "# dataset = \"flowers\"\n",
        "# with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
        "#     train_flowers = pkl.load(file)\n",
        "#     print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Splitting FLOWERS dataset\n",
        "\n",
        "# X_train, Y_train, X_val, Y_val = split_data(train_flowers[0], train_flowers[1], train_ratio=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Fitting flowers dataset\n",
        "\n",
        "# fit(X_train, Y_train, 'flowers')\n",
        "\n",
        "# # Accuracy for fitting\n",
        "# Y_test = predict(X_train, 'flowers')\n",
        "# accuracy = np.sum(Y_test == Y_train)/len(Y_test)\n",
        "# print(\"Training percentage accuracy:\", accuracy * 100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Predict for FLOWERS\n",
        "\n",
        "# Y_test = predict(X_val, 'flowers')\n",
        "# accuracy = np.sum(Y_test == Y_val)/len(Y_test)\n",
        "# print(\"Predicting percentage accuracy :\", accuracy*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = \"mnist\"\n",
        "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
        "    train_mnist = pkl.load(file)\n",
        "    print(\n",
        "        f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
        "\n",
        "# fit(train_mnist[0], train_mnist[1], 'mnist')\n",
        "\n",
        "dataset = \"flowers\"  # \"mnist\"/\"flowers\"\n",
        "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
        "    train_flowers = pkl.load(file)\n",
        "    print(\n",
        "        f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
        "\n",
        "# fit(train_flowers[0], train_flowers[1], 'flowers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Predict for mnist and flowers\n",
        "\n",
        "# Y_test = predict(train_mnist[0], 'mnist')\n",
        "# accuracy = np.sum(Y_test == train_mnist[1])/len(Y_test)\n",
        "# print(\"Predicting percentage accuracy :\", accuracy*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# Y_test = predict(train_flowers[0], 'flowers')\n",
        "# accuracy = np.sum(Y_test == train_flowers[1])/len(Y_test)\n",
        "# print(\"Predicting percentage accuracy :\", accuracy*100, \"%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
